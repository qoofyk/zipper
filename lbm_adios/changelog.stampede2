# Change Log
All notable changes to this project will be documented in this file.
The format is based on [Keep a Changelog](http://keepachangelog.com/) 
and this project adheres to [Semantic Versioning](http://semver.org/).
TODO
	see github log
	* redesigned flexpath
	* new version of adios

	0. use environment variable instead of preprocessor
	1. nnti in flexpath, not usable.
	2. dspaces hanging
	3. ENABLE_TESTING in flexpath conf, enabled

## Jan 4
	* problem fix:
		- longer analysis time in stampede: run_analysis is compiled as
		dynamic library to accomadate decaf

## Jan 3
	* question: analysis time is longer than yuankun's 40s v 20s
	* end2end time is 60s+ if using more link processes 
	* decaf is linked with lbm, end2end time  78s for 64v32, now use nmoments
	=4
	* decaf is connected with lbm and 100 steps runs for 87s(not using
	optimizing flags)
	* decaf added to the source tree, cmake files are now in cmake/
	* buffer operation is now in seperate objects, also 38s per step

## Jan 2
	now buffer is seperated from lbm
## Dec 30
	Use valgrind to debug
	valgrind --leak-check=yes ./lbm 1 10 &> run.log
	grep -R run.log Invalid, will tell segmenta fault



## Dec 29
	Doing
		modulize
	* Lammps and lBM should have been better integrated:
	- the decaf examples/C/lammps for example:
		* it uses the lammps library interface
		* the input file doesn't call "run" and "dump"
		* this examples gatter all atoms, however it can be better, see
		lammps_gather_atoms for more
		details.(https://github.com/lammps/lammps/blob/master/src/lammps.h\c)
		
	- lbm code can also be replaced with better architecture.


## Dec 18
	flexpath fixed: remove data description, otherwise seg fault(didn't happen
	in 1.12)
		see flex_debug2 to see more details
	todo:
		1. use 3d dimension and sliced 
		2. try skeleton code check timing, 400s!
## Dec 16
	new commit:
		* add avx wall flags, consistent with yuankun's exp
		* clean anaylsis code
		* add flexpath debug

## Dec 15
	need to use ddt to look into :
		https://software.intel.com/en-us/forums/intel-clusters-and-hpc-technology/topic/329053
	use flexpath_debug

## Dec 14
	using flexpath 1.13 fixing memory usage
	but integrating 1.13 with my app, consumer had issue

## Oct 28
		cm_blocking read
		INT_CMstart_read_thread
		use_read_thread
## Oct 27
	366616
		rerun flexpthpath 34v17, 
	365308
		8vs4 development, correct
## Oct 16
add trace support
	HAS_TRACE=1 then submit jobs
	343400 is an example
	
This version generates the results for performance comparsion(8vs4)
	adios_dspaces:137.4
	adios_dimes:111.2
	adios_mpiio:96.38
	adios_flexpath:43.95
	native_dspaces:80.52
	native_dimes:51.27
	zipper:34.8
	lbm only: 33.5

	flexpath has much better performance when there are less processes in each
	node:
	using 3 node, if the processes are
	8vs4: 43.95s
	34vs17 220.76s
	68vs34 511.4s

## Oct 12
	stripe size=-1 not obvious 
	flexpath is slow at first but has good scalibility
## Oct 10
	stampede2 is not stable in stampede2.
## Oct 8
	flexpath/mpiio 512vs256
## Oct 6
#### added
	dspaces test added in project, dspaces server has rdma_bind_addr err with
	4 server nodes


## Oct 5
#### info
- now use mpiexec.hydra with new process layouts
- ibrun info
	export TACC_IBRUN_DEBUG=1
	less /home1/04446/tg837458/Downloads/LaucherTest/results/308504/app2.log
	less ~/.slurm/job.308504.hostlist.Vri26bxz

## Oct 3
	* stampded maintainance, will come back to bridges
	* dsexample cannot run multiple applications with intel mpirun, 1814068:
		run: Job step creation temporarily disabled, retrying
	* try launchertest:
		mpirun has only output for only one app



## Oct 2
#### added
- dspace example now runs in stampede2, results to verify
- dspace/flexpath/adios installed. Note withouth avx flags!

#### exp
	small experiments can run,
	larger experiment need specifiy layout, 


