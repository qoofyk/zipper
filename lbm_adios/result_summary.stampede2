## Oct 12
experiment
    328792
        flexpath with 272vs136, stopped at step 97, see consumer_32.clog
    328263
        use stripe size=-1, and rerun 68vs34 mpiio
JOBS
    327488
        development queue with env variables in env_dev.log
    327475
        normal queue with envriables output in env_normal.log
## Oct 11
JOBS
    use idev interactive session:
        *. dev queue
            1. run the server with
                mpiexec.hydra  -n 4 /work/04446/tg837458/stampede2/envs/Dataspacesroot/bin/dataspaces_server -s 4 -c 48 &> results/dev_server.log &
            2. run writer with
                mpiexec.hydra  -n 32 /work/04446/tg837458/stampede2/envs/Dataspacesroot/bin/test_writer DATASPACES 32 3 2 4 4 128 64 64 10 1 &> results/dev_writer.log  &
            3. run the reader with
                mpiexec.hydra  -n 16 /work/04446/tg837458/stampede2/envs/Dataspacesroot/bin/test_reader DATASPACES 16  3 2 2 4 128 128 64 10 2 &> results/dev_reader.log &
        * compute queue, run the server with 
            mpiexec.hydra  -n 4 /work/04446/tg837458/stampede2/envs/Dataspacesroot/bin/dataspaces_server -s 4 -c 48 &> results/compute_server.log &
                rdma_bind_addr error imediately 

    324279:
       development queue with output ldd info
    324337:
        normal queue with output ldd info
    
## Oct 10
# conclusion
    1. rpc_bind_addr always happens in normal queue
    2. 
continue address the normal queue
    321133
        ds-test-tense with 8 nodes, each with 64 procs, normal, rdma_bind_addr err in server!
    321122
        ds-test-tense with 8 nodes, each with 64 procs, rpc_connect error, development
   

NOTE: same  scripts of ds-test.skel, sometimes with rpc_connect erro
    321035
        development with 3 nodes, 
    320983
        development queue, fine
    320997
        normal queue, rdma err
## Oct 8
JOBS libfabrics

    318997
        flexpath, slower than disk!!
    
    318996
        mpiio
    ------------------------------------------------
    above has 512/256, see google doc for more details

	318948
		flexpath 196
	318892
		flexpath now use same adios_staging with dspaces
		correct

    
## Oct 6
JOBS
    311613
        development queue can run

    311586
        normal queue cannot run

    using impi again
    --------------------------------------
    311147
    dataspaces shipped test program, with 8 server nodes, rdma_bind_addr
    310980
    run native-adios 196 with mvapich dspaces built, still has rdma_bind_addr
    
    310961
    using mvapich in stampded with with-infiband-interface=ib0
        app cp is full.
        'dsrpc_cn_register()' failed with -12.
    * solved: add DATASPACES as an auument

## Oct 5
JOBS
    310262
        196vs98 with native_dspace, rdma_bind_addr err
    310256
        8vs4 with native_dspaces

    above starts using new lanucher with machinefile
    --------------------------------
    
    309374
        8vs4 with mpiio
        correctly run
    
    
    309206
        196vs98 dspaces_nokeep
        server cannot init:
            rdma_bind_addr -1 in rpc_server_init.
            'rpc_server_init()': failed with -1.

    309158
        8vs4 dspaces_nokeep
        use new machinefile layout
    
    308648
        196vs98
        process layout
        server procs(28*4 procs are packed into two nodes)
## Oct 2
JOBS
    303418 native staging  196vs 98, rpc bind err

    303396 adios_staging of dspaces works
    303356 native_staging of dpsaces, works
    303350 mpiio successfully run, works

