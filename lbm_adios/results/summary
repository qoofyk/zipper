Mar 4
1114745
    24-12 300 mpiio with read timer
    wait for results
1114063
    24:12 dspaces with 16sp in 4 server node
    use 20 maxversion, lockname is associated with mode 20
    wait for results

1113082
    24:12 mpiio in 300 steps, 
    923s e2e time, 
    there is a pause in consumer at No.278 step 
1113054
    24:12 mpiio in 100 steps, consumer and producer finishs almost at the same time
    128s e2e time
    
    
1112967
    24:12 dspaces with 16 sp in 4 server node

1112935
    24:12 mpiio in 300 steps
1112767
    24:12 
    raw spaces with 8 sp in 2 server node
    900+ end2end
    
    
1112728
    24:12
    simulation only
    121s for 300 steps

    
Mar 3
1112615
    timestep changed to 300
    need more max_verion in configurations
1109252
    8 servers in two nodes
    producer
        t_prepare:0.078760 s, t_cal 31.777218 s,t_buffer = 0.369984, t_write 14.020842 s
    consumer 
        time for read 52.290949 s; time for ds_get 43.102167 s; time for analyst 37.557888 s

1108717
    8 servers in one node
    wait for results
    producer
        rank 0, t_prepare:0.072677 s, t_cal 31.705974 s,t_buffer = 0.334619, t_write 14.063485 s
    consumer
        stat:time for read 52.457892 s; time for ds_get 42.482243 s; time for analyst 36.992608 s
1108676
    four servers in one nodes
    wait for results
    producer:
        t_prepare:0.079526 s, t_cal 32.637577 s,t_buffer = 0.372480, t_write 15.397109 s
    consumer
        time for read 49.933296 s; time for ds_get 39.106010 s; time for analyst 37.426931 s
1108502
    try 1108484 again
    producer
        rank 0, t_prepare:0.076126 s, t_cal 31.894343 s,t_buffer = 0.340155, t_write 21.663085 s
    consumer
        stat:time for read 71.426228 s; time for ds_get 48.901551 s; time for analyst 37.309971 s
1108484
    configuration same as 1108421
    use 2 server processes
    note: need to roll back!
    wait for results
    producer
         is okay and even faster
    consumer
        stoped at the 47th step
    server info:
        Status (12) is not IBV_WC_SUCCESS.
        (__process_event): err (-12).
1108421
    1 server processes
    timer added in raw_dspaces version
    producer:
        rank 0, t_prepare:0.071743 s, t_cal 32.427348 s,t_buffer = 0.342315, t_write 31.190054 s
    consumer dominate the execution timer
        stat:time for read 80.504931 s; time for ds_get 54.705117 s; time for analyst 36.792386 s
May 2
1093155
    raw dimes version first submit
    wait for results
    dimes_memory_alloc(): ERROR: no sufficient RDMA memory for caching data with 16777216 bytes. Suggested fix: increase the value of '--with-dimes-rdma-buffer-size' at configuration
    default is 64M for each processes, can only 16M*4=64M, the fifth step will generate this error
1092629
    raw dspaces getting more info
    wait for results
    correct, just need to add timer to break down time
-------------------------------------------
-
1086067
    version changed to 5, cannot unlock in first reader iteration, producer is also slower

1085860
    working version of raw dspaces
    0.1~0.2s per step in writting

1085794
	remove gdimdefine functions
	global dims set to 2 in jobfile => pmi error

Q   
    gdim removed?
1084745
    only one process tried to release read lock, did others return?kh
1082998
    see what will happens if using same lock
    consumer still cannot read
    changed it back
1081962
    remove dspace_put_sync
    first serveral writes are very fast
    problem:consumer can get read lock but didn't take any action
1073688
    increase maxversion to 100
    still 5s/step
    problem:consumer can get read lock but didn't take any action
    

1072840
    dspaces is slow!
    5s/step
    maxversion =5
    problem:consumer can get read lock but didn't take any action



-------------------------------------------------------------------------------
recover:
    1. remove branch in insert_into_adios, still blocked at second step
    2. add max-buffer-size and group-size back

5bf287a8159489d335d6fccb6f0ffdbe7defd7fa works.
April 24: dspaces nokeep cannot start second step
--------------------------------------------------------------
1023888
    April 24
    queue size =30
    t_prepare:0.096793 s, t_cal 58.699383 s,t_buffer = 0.391298, t_write 67.074677 s
    stat:time for read 80.986417 s; time for advancing step 60.186810 s; time for analyst 42.728954 s
    
    
1023832
    only use queue size =100 use current
    similar to 1023483, when producer finishes, only 50 iteration are finished in consumer
1023284
    use ADIOS_LOCK_NONE for flexpath, and also use queue size =100
    writting time is  reduced but reading time and simulation time is increased
    rank 0: t_prepare:0.096157 s, t_cal 74.112131 s,t_buffer = 0.378756, t_write 18.409973 s
    consumer:
        read 95.798134 s; time for advancing step 55.317296 s; time for analyst 38.530918 s
    
    previous(1020273):
        rank 0, t_prepare:0.117190 s, t_cal 34.793658 s,t_buffer = 0.363246, t_write 48.276190 s
        time for read 31.993267 s; time for advancing step 13.858851 s; time for analyst 42.458436 s


1022954
    use ADIOS_LOCK_NONE for dataspaces, cannot start the second step
--------------------------------------------------------------------
2 vs 2
    2:2 for dataspaces
    can only run 1 step
        obj_put_update_dht()': this should not happen, num_de == 0 ?!
1020425
    2:2 for flexpath
    rank 0, t_prepare:0.100240 s, t_cal 36.241775 s,t_buffer = 0.352581, t_write 35.290532 s
1020431
    2:2 for mpiio
    rank 0, t_prepare:0.094998 s, t_cal 34.470667 s,t_buffer = 0.380254, t_write 41.995600 s

    
--------------------------------------------------------------------
2 vs 1
1020409
    2:1 for dataspacs
    much slower, write time, same as 8:4
    rank 1, t_prepare:0.090932 s, t_cal 35.855491 s,t_buffer = 0.694799, t_write 148.333942 s
1020273
    2:1 for flexpath
    t_prepare:0.117190 s, t_cal 34.793658 s,t_buffer = 0.363246, t_write 48.276190 s

1020351
    2:1 for mpiio
    rank 0, t_prepare:0.095643 s, t_cal 33.620270 s,t_buffer = 0.408753, t_write 41.847312 s


--------------------------------------------------------------------
1 vs 1
1020171
    April 20
    1 proc for producer /consumer in dimes
    rank 0, t_prepare:0.099800 s, t_cal 39.225544 s,t_buffer = 0.428297, t_write 59.117687 s
    
1019194
    April 19
    1 proc for producer /consumer in dataspaces
    rank 0, t_prepare:0.115774 s, t_cal 38.675447 s,t_buffer = 0.464524, t_write 59.304806 s
    
1019183
    April 19
    use 1 proc for producer/consumer in mpiio
    rank 0, t_prepare:0.102954 s, t_cal 29.752500 s,t_buffer = 0.337182, t_write 35.613051 s
1019177
    April 19
    use 1 proc for producer/consumer in flexpath
        writing time is much less(in each step only 0.1s+ needed instead 1s+)
        rank 0, t_prepare:0.112377 s, t_cal 37.403693 s,t_buffer = 0.418905, t_write 13.139727 s

    second try(1019213)
        rank 0, t_prepare:0.114158 s, t_cal 38.601449 s,t_buffer = 0.463431, t_write 11.460353 s


        
-------------------------------------------------------------------
1013543
    config
        export nnti
        QUEUE_SIZE=10
    
    first several iterations are very fast
    t_prepare:0.099772 s, t_cal 48.264999 s,t_buffer = 0.365479, t_write 122.166682 s
1000659
    flexpath again
    t_prepare:0.099056 s, t_cal 47.947185 s,t_buffer = 0.381423, t_write 133.391801 s
    
1000611
    dspaces nokeep 8:4 same as before, 33s cal, total 147s
1000598
    flexpath 8:4
    t_prepare:0.099268 s, t_cal 42.772855 s,t_buffer = 0.388137, t_write 149.501805 s(why calculation is longer?)
-------------------------------------------------------------------
990028
    mpiio for 768-384
989977
    1 step has writting time 10s
    2 step has 5s
    after that, each step is around 1.4s
989971
    print out the time spend in each step

985605
    2 server nodes with  8 processes
    t_prepare:0.070657 s, t_cal 33.336477 s, t_write 123.082560 s
    time for read 41.152115 s; time for advancing step 69.784354 s; time for analyst 38.104001 s
    
985462
    2 nodes with 4 server processes
    t_prepare:0.083320 s, t_cal 33.259625 s, t_write 114.416353 s(different with the experiment when run_analys is comment out!!)
    stat:time for read 31.125366 s; time for advancing step 69.925751 s; time for analyst 38.699015 s

985138
    Using 1 node with 4 server processes
    t_prepare:0.069294 s, t_cal 34.124658 s, t_write 121.832758 s
    stat:time for read 35.299027 s; time for advancing step 75.014883 s; time for analyst 38.248347 s
985141
    1 server node with 1 server process
    t_prepare:0.082188 s, t_cal 32.822827 s, t_write 147.587935 s(will decrease to 120 if disable analysis, 989763)
    stat:time for read 53.557231 s; time for advancing step 76.971237 s; time for analyst 39.008861 s

-------------------------------------------------------------------
run_analysis is commented since 982346()

##983952
##    use two nodes and 8 server procs
##    t_prepare:0.070522 s, t_cal 32.412648 s, t_write 85.629872 
##983941
##    mpiio for 192:96
##        correct
##        t_prepare:0.102406 s, t_cal 52.585197 s, t_write 188.630988 s
##983987
##    mpiio for 48:24
##    166s for 100 iterations
##
##----------------------------------------------------------------
##Dspace without keep 
##---------------------------------------------------------------
##983916
##    use two nodes and 4 server procs
##
##    t_prepare:0.072535 s, t_cal 33.617150 s, t_write 81.061058 s
##
##983887
##    Use 4 server procs in 1 node(885 for 2 procs, 889 for 8 procs)
##    producer
##        rank 0, t_prepare:0.073306 s, t_cal 32.921603 s, t_write 85.992348 s
##    consumer
##        stat:time for read 36.048645 s; time for advancing step 74.303108 s; time for analyst 0.000015 s 
##983861 
##    increase xml buffer size to 100M
##    producer
##        rank 0, t_prepare:0.072356 s, t_cal 33.428042 s, t_write 120.205607 s
##    consumer
##        stat:time for read 57.506153 s; time for advancing step 83.994528 s; time for analyst 0.000015 s
##982346
##    Producer 
##        rank 0, t_prepare:0.088599 s, t_cal 33.367076 s, t_write 119.299031 s
##    Consumer
##        stat:time for read 49.542193 s; time for advancing step 90.988737 s; time for analyst 0.000019 s
        
978592
    rank 0, t_prepare:0.071245 s, t_cal 33.226557 s, t_write 149.790138 s 
977946
    max_version =2;
    185s.

977829
    Previous using max_version=5
    now changed to 1
    April 3 12:30
    8 vs 4 
-------------------------------------
976485
    April 3 09:35
    8 vs 4 dimes w/o keep
    138s

976473
    April 3 08:59
    8 vs 4 sim_only 
    33s

876434
    April 3 08:56
    8 vs 4 dspaecs w/o keep
    179s

975029
    April 2 17:46
    8 vs 4 MPIIO
    98.8s
970196
    Mar 31 15:27
    Use -O3 can reduce running time significantly
-----------------------------------------------------------------
didn't use O3
966796
    Mar 30 15:51
    1 node for dspace servers, 48_24, max version 1
966766
    mar 30 14:44
    192 simulation procs only in 100 timesteps, 144.42s
965971
    Mar 30 13:36
    48 sim procs and 24 analysis procs with dspaces, 74 steps(increased from 41) in 5 min
    
965907
    Mar 30 12:24
    48 sim procs and 24 analysis procs with dimes, 87 steps in 5 min
    Q: does increase dspace server node help?
    
965855
    Mar 30 12:14
    48 sim procs only, 141.18s  
965780
    Mar 30 11:59
    48 sim procs and 24 analysis procs with dspaces, 41 steps in 5 min
    
965712
    Mar 30 11:37
    sim_only 121.81
965450
    Mar 30 11:06
    dimes with keep 172.6
964999
    Mar 30 10:31
    dimes with nokeep 171.40
964902
    Mar 30 10:07
    dspaces with keep 8:4, 197.65
964613
    Mar 30 00:30
    correctly get node list in bridges. continuous 100 timesteps in bridges with correct process layout
    185.69
8531205
    Mar 29
    node file fixed, now all application run in different nodes
8394708
    Mar 24
    1/1 proc correct, but most data is zero
8426008
    mar 25
    4/8for dspaces transport

8426034
    Mar 25
    4/8 for dimes transport
8439991
    Mar 25, 19:10
    use scrach for .pb file, now mpiio is okay!
8440002
    Mar 25 19;16
    if data size too large, need larger buffer to fit all data
8445129
    Mar 26 13:10
    if adding  keeping,(additional init_read_method() in reader application) reader no actions
8445202
    Mar 26 13:42
    move adios_init_read before adios_init, reader cannot stop(fixed by store errno of streaming read)
8445616
    mar 26 15:08
    keep written into same file, (mode "w")
8445921
    Mar 26 15:46
    keep only happens since second iteration

8522408
    Mar 29 09:50
    Use 
        CMD_SERVER="mpirun_rsh -hostfile $SLURM_NODEFILE -n $PROCS_SERVER ${DS_SERVER} -s $PROCS_SERVER -c $DS_CLIENT_PROCS"
        $CMD_SERVER  &> ${PBS_RESULTDIR}/server.log &
        echo "server applciation lauched: $CMD_SERVER" 
    server is launched, but doesn't generate conf file, so clinets cannot start

8522493
    Mar 29 09:57
    Use 
        # Run DataSpaces servers      
        LAUNCHER=ibrun                 
        CMD_SERVER="$LAUNCHER -hostfile $SLURM_NODEFILE -n $PROCS_SERVER ${DS_SERVER} -s $PROCS_SERVER -c $DS_CLIENT_PROCS"
        $CMD_SERVER  &> ${PBS_RESULTDIR}/server.log &
        echo "server applciation lauched: $CMD_SERVER"

    Result
        ibrun doesn't support hostfile option
        Unkown option hostfile
    To do 
        looking into ibrun scripts in /opt/sdsc/bin/ibrun


    
