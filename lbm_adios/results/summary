1000659
    flexpath again
    t_prepare:0.099056 s, t_cal 47.947185 s,t_buffer = 0.381423, t_write 133.391801 s
    
1000611
    dspaces nokeep 8:4 same as before, 33s cal, total 147s
1000598
    flexpath 8:4
    t_prepare:0.099268 s, t_cal 42.772855 s,t_buffer = 0.388137, t_write 149.501805 s(why calculation is longer?)
-------------------------------------------------------------------
990028
    mpiio for 768-384
989977
    1 step has writting time 10s
    2 step has 5s
    after that, each step is around 1.4s
989971
    print out the time spend in each step

985605
    2 server nodes with  8 processes
    t_prepare:0.070657 s, t_cal 33.336477 s, t_write 123.082560 s
    time for read 41.152115 s; time for advancing step 69.784354 s; time for analyst 38.104001 s
    
985462
    2 nodes with 4 server processes
    t_prepare:0.083320 s, t_cal 33.259625 s, t_write 114.416353 s(different with the experiment when run_analys is comment out!!)
    stat:time for read 31.125366 s; time for advancing step 69.925751 s; time for analyst 38.699015 s

985138
    Using 1 node with 4 server processes
    t_prepare:0.069294 s, t_cal 34.124658 s, t_write 121.832758 s
    stat:time for read 35.299027 s; time for advancing step 75.014883 s; time for analyst 38.248347 s
985141
    1 server node with 1 server process
    t_prepare:0.082188 s, t_cal 32.822827 s, t_write 147.587935 s(will decrease to 120 if disable analysis, 989763)
    stat:time for read 53.557231 s; time for advancing step 76.971237 s; time for analyst 39.008861 s

-------------------------------------------------------------------
run_analysis is commented since 982346()

##983952
##    use two nodes and 8 server procs
##    t_prepare:0.070522 s, t_cal 32.412648 s, t_write 85.629872 
##983941
##    mpiio for 192:96
##        correct
##        t_prepare:0.102406 s, t_cal 52.585197 s, t_write 188.630988 s
##983987
##    mpiio for 48:24
##    166s for 100 iterations
##
##----------------------------------------------------------------
##Dspace without keep 
##---------------------------------------------------------------
##983916
##    use two nodes and 4 server procs
##
##    t_prepare:0.072535 s, t_cal 33.617150 s, t_write 81.061058 s
##
##983887
##    Use 4 server procs in 1 node(885 for 2 procs, 889 for 8 procs)
##    producer
##        rank 0, t_prepare:0.073306 s, t_cal 32.921603 s, t_write 85.992348 s
##    consumer
##        stat:time for read 36.048645 s; time for advancing step 74.303108 s; time for analyst 0.000015 s 
##983861 
##    increase xml buffer size to 100M
##    producer
##        rank 0, t_prepare:0.072356 s, t_cal 33.428042 s, t_write 120.205607 s
##    consumer
##        stat:time for read 57.506153 s; time for advancing step 83.994528 s; time for analyst 0.000015 s
##982346
##    Producer 
##        rank 0, t_prepare:0.088599 s, t_cal 33.367076 s, t_write 119.299031 s
##    Consumer
##        stat:time for read 49.542193 s; time for advancing step 90.988737 s; time for analyst 0.000019 s
        
978592
    rank 0, t_prepare:0.071245 s, t_cal 33.226557 s, t_write 149.790138 s 
977946
    max_version =2;
    185s.

977829
    Previous using max_version=5
    now changed to 1
    April 3 12:30
    8 vs 4 
-------------------------------------
976485
    April 3 09:35
    8 vs 4 dimes w/o keep
    138s

976473
    April 3 08:59
    8 vs 4 sim_only 
    33s

876434
    April 3 08:56
    8 vs 4 dspaecs w/o keep
    179s

975029
    April 2 17:46
    8 vs 4 MPIIO
    98.8s
970196
    Mar 31 15:27
    Use -O3 can reduce running time significantly
-----------------------------------------------------------------
didn't use O3
966796
    Mar 30 15:51
    1 node for dspace servers, 48_24, max version 1
966766
    mar 30 14:44
    192 simulation procs only in 100 timesteps, 144.42s
965971
    Mar 30 13:36
    48 sim procs and 24 analysis procs with dspaces, 74 steps(increased from 41) in 5 min
    
965907
    Mar 30 12:24
    48 sim procs and 24 analysis procs with dimes, 87 steps in 5 min
    Q: does increase dspace server node help?
    
965855
    Mar 30 12:14
    48 sim procs only, 141.18s  
965780
    Mar 30 11:59
    48 sim procs and 24 analysis procs with dspaces, 41 steps in 5 min
    
965712
    Mar 30 11:37
    sim_only 121.81
965450
    Mar 30 11:06
    dimes with keep 172.6
964999
    Mar 30 10:31
    dimes with nokeep 171.40
964902
    Mar 30 10:07
    dspaces with keep 8:4, 197.65
964613
    Mar 30 00:30
    correctly get node list in bridges. continuous 100 timesteps in bridges with correct process layout
    185.69
8531205
    Mar 29
    node file fixed, now all application run in different nodes
8394708
    Mar 24
    1/1 proc correct, but most data is zero
8426008
    mar 25
    4/8for dspaces transport

8426034
    Mar 25
    4/8 for dimes transport
8439991
    Mar 25, 19:10
    use scrach for .pb file, now mpiio is okay!
8440002
    Mar 25 19;16
    if data size too large, need larger buffer to fit all data
8445129
    Mar 26 13:10
    if adding  keeping,(additional init_read_method() in reader application) reader no actions
8445202
    Mar 26 13:42
    move adios_init_read before adios_init, reader cannot stop(fixed by store errno of streaming read)
8445616
    mar 26 15:08
    keep written into same file, (mode "w")
8445921
    Mar 26 15:46
    keep only happens since second iteration

8522408
    Mar 29 09:50
    Use 
        CMD_SERVER="mpirun_rsh -hostfile $SLURM_NODEFILE -n $PROCS_SERVER ${DS_SERVER} -s $PROCS_SERVER -c $DS_CLIENT_PROCS"
        $CMD_SERVER  &> ${PBS_RESULTDIR}/server.log &
        echo "server applciation lauched: $CMD_SERVER" 
    server is launched, but doesn't generate conf file, so clinets cannot start

8522493
    Mar 29 09:57
    Use 
        # Run DataSpaces servers      
        LAUNCHER=ibrun                 
        CMD_SERVER="$LAUNCHER -hostfile $SLURM_NODEFILE -n $PROCS_SERVER ${DS_SERVER} -s $PROCS_SERVER -c $DS_CLIENT_PROCS"
        $CMD_SERVER  &> ${PBS_RESULTDIR}/server.log &
        echo "server applciation lauched: $CMD_SERVER"

    Result
        ibrun doesn't support hostfile option
        Unkown option hostfile
    To do 
        looking into ibrun scripts in /opt/sdsc/bin/ibrun


    
